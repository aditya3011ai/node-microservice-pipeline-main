
==> Audit <==
|-----------|---------------------|----------|--------|---------|---------------------|---------------------|
|  Command  |        Args         | Profile  |  User  | Version |     Start Time      |      End Time       |
|-----------|---------------------|----------|--------|---------|---------------------|---------------------|
| start     |                     | minikube | aditya | v1.36.0 | 27 Jun 25 08:32 IST |                     |
| start     |                     | minikube | aditya | v1.36.0 | 30 Jun 25 16:03 IST |                     |
| start     |                     | minikube | aditya | v1.36.0 | 30 Jun 25 16:11 IST | 30 Jun 25 16:16 IST |
| start     |                     | minikube | aditya | v1.36.0 | 01 Jul 25 16:07 IST | 01 Jul 25 16:07 IST |
| service   | nginx-service --url | minikube | aditya | v1.36.0 | 01 Jul 25 16:50 IST | 01 Jul 25 16:51 IST |
| service   | nginx-service --url | minikube | aditya | v1.36.0 | 01 Jul 25 17:00 IST | 01 Jul 25 17:03 IST |
| service   | nginx-service --url | minikube | aditya | v1.36.0 | 01 Jul 25 17:03 IST | 01 Jul 25 17:12 IST |
| start     |                     | minikube | aditya | v1.36.0 | 03 Jul 25 13:11 IST | 03 Jul 25 13:12 IST |
| start     |                     | minikube | aditya | v1.36.0 | 09 Jul 25 08:29 IST | 09 Jul 25 08:29 IST |
| dashboard |                     | minikube | aditya | v1.36.0 | 09 Jul 25 08:30 IST |                     |
| start     |                     | minikube | aditya | v1.36.0 | 11 Jul 25 09:35 IST | 11 Jul 25 09:35 IST |
| ip        |                     | minikube | aditya | v1.36.0 | 11 Jul 25 09:39 IST | 11 Jul 25 09:39 IST |
| ip        |                     | minikube | aditya | v1.36.0 | 11 Jul 25 09:41 IST | 11 Jul 25 09:41 IST |
| dashboard |                     | minikube | aditya | v1.36.0 | 11 Jul 25 09:43 IST |                     |
| start     |                     | minikube | aditya | v1.36.0 | 11 Jul 25 12:31 IST | 11 Jul 25 12:32 IST |
| dashboard |                     | minikube | aditya | v1.36.0 | 11 Jul 25 12:35 IST |                     |
| start     |                     | minikube | aditya | v1.36.0 | 16 Jul 25 20:24 IST | 16 Jul 25 20:25 IST |
| start     |                     | minikube | aditya | v1.36.0 | 17 Jul 25 08:51 IST |                     |
| start     | --driver=docker     | minikube | aditya | v1.36.0 | 17 Jul 25 08:51 IST | 17 Jul 25 08:52 IST |
| addons    | enable ingress      | minikube | aditya | v1.36.0 | 17 Jul 25 08:52 IST | 17 Jul 25 08:53 IST |
| start     |                     | minikube | aditya | v1.36.0 | 17 Jul 25 09:17 IST |                     |
| stop      |                     | minikube | aditya | v1.36.0 | 17 Jul 25 09:18 IST | 17 Jul 25 09:18 IST |
| start     | --driver=docker     | minikube | aditya | v1.36.0 | 17 Jul 25 09:18 IST |                     |
|-----------|---------------------|----------|--------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/07/17 09:18:53
Running on machine: ISKON-Krishna
Binary: Built with gc go1.24.0 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0717 09:18:53.312529   33734 out.go:345] Setting OutFile to fd 1 ...
I0717 09:18:53.312664   33734 out.go:397] isatty.IsTerminal(1) = true
I0717 09:18:53.312669   33734 out.go:358] Setting ErrFile to fd 2...
I0717 09:18:53.312673   33734 out.go:397] isatty.IsTerminal(2) = true
I0717 09:18:53.312897   33734 root.go:338] Updating PATH: /home/aditya/.minikube/bin
W0717 09:18:53.313011   33734 root.go:314] Error reading config file at /home/aditya/.minikube/config/config.json: open /home/aditya/.minikube/config/config.json: no such file or directory
I0717 09:18:53.313342   33734 out.go:352] Setting JSON to false
I0717 09:18:53.314922   33734 start.go:130] hostinfo: {"hostname":"ISKON-Krishna","uptime":4540,"bootTime":1752719594,"procs":66,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.167.4-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"1dd81229-5b48-4901-a4ff-ca6a8a1d8127"}
I0717 09:18:53.314977   33734 start.go:140] virtualization:  guest
I0717 09:18:53.319007   33734 out.go:177] 😄  minikube v1.36.0 on Ubuntu 22.04 (amd64)
I0717 09:18:53.324713   33734 notify.go:220] Checking for updates...
I0717 09:18:53.325054   33734 out.go:177]     ▪ KUBECONFIG=/var/jenkins_home/.kube/config
I0717 09:18:53.335778   33734 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0717 09:18:53.336575   33734 driver.go:404] Setting default libvirt URI to qemu:///system
I0717 09:18:53.402095   33734 docker.go:123] docker version: linux-28.3.0:Docker Engine - Community
I0717 09:18:53.402248   33734 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0717 09:18:53.513719   33734 info.go:266] docker info: {ID:2dd340f3-6e03-44e0-a0b1-04fa633d36d3 Containers:5 ContainersRunning:4 ContainersPaused:0 ContainersStopped:1 Images:8 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:70 OomKillDisable:true NGoroutines:80 SystemTime:2025-07-17 09:18:53.499566241 +0530 IST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4033732608 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:ISKON-Krishna Labels:[] ExperimentalBuild:false ServerVersion:28.3.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.37.3]] Warnings:<nil>}}
I0717 09:18:53.513800   33734 docker.go:318] overlay module found
I0717 09:18:53.515945   33734 out.go:177] ✨  Using the docker driver based on existing profile
I0717 09:18:53.517637   33734 start.go:304] selected driver: docker
I0717 09:18:53.517647   33734 start.go:908] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/aditya:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0717 09:18:53.517719   33734 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0717 09:18:53.517823   33734 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0717 09:18:53.614574   33734 info.go:266] docker info: {ID:2dd340f3-6e03-44e0-a0b1-04fa633d36d3 Containers:5 ContainersRunning:4 ContainersPaused:0 ContainersStopped:1 Images:8 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:70 OomKillDisable:true NGoroutines:80 SystemTime:2025-07-17 09:18:53.596760021 +0530 IST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4033732608 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:ISKON-Krishna Labels:[] ExperimentalBuild:false ServerVersion:28.3.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.37.3]] Warnings:<nil>}}
I0717 09:18:53.615027   33734 cni.go:84] Creating CNI manager for ""
I0717 09:18:53.615068   33734 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0717 09:18:53.615114   33734 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/aditya:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0717 09:18:53.617132   33734 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0717 09:18:53.618889   33734 cache.go:121] Beginning downloading kic base image for docker with docker
I0717 09:18:53.620586   33734 out.go:177] 🚜  Pulling base image v0.0.47 ...
I0717 09:18:53.622116   33734 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0717 09:18:53.622168   33734 preload.go:146] Found local preload: /home/aditya/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0717 09:18:53.622178   33734 cache.go:56] Caching tarball of preloaded images
I0717 09:18:53.622257   33734 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0717 09:18:53.622297   33734 preload.go:172] Found /home/aditya/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0717 09:18:53.622309   33734 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0717 09:18:53.622416   33734 profile.go:143] Saving config to /home/aditya/.minikube/profiles/minikube/config.json ...
I0717 09:18:53.777058   33734 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon, skipping pull
I0717 09:18:53.777074   33734 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in daemon, skipping load
I0717 09:18:53.777121   33734 cache.go:230] Successfully downloaded all kic artifacts
I0717 09:18:53.777153   33734 start.go:360] acquireMachinesLock for minikube: {Name:mkb8c9eb16693e5e6ef2dad279bfabac0c84c2c1 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0717 09:18:53.777269   33734 start.go:364] duration metric: took 76.34µs to acquireMachinesLock for "minikube"
I0717 09:18:53.777290   33734 start.go:96] Skipping create...Using existing machine configuration
I0717 09:18:53.777297   33734 fix.go:54] fixHost starting: 
I0717 09:18:53.777766   33734 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0717 09:18:53.826364   33734 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0717 09:18:53.826390   33734 fix.go:138] unexpected machine state, will restart: <nil>
I0717 09:18:53.843996   33734 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0717 09:18:53.845877   33734 cli_runner.go:164] Run: docker start minikube
I0717 09:18:54.612645   33734 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0717 09:18:54.646313   33734 kic.go:430] container "minikube" state is running.
I0717 09:18:54.650896   33734 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0717 09:18:54.695588   33734 profile.go:143] Saving config to /home/aditya/.minikube/profiles/minikube/config.json ...
I0717 09:18:54.695972   33734 machine.go:93] provisionDockerMachine start ...
I0717 09:18:54.696086   33734 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0717 09:18:54.734111   33734 main.go:141] libmachine: Using SSH client type: native
I0717 09:18:54.734443   33734 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0717 09:18:54.734454   33734 main.go:141] libmachine: About to run SSH command:
hostname
I0717 09:18:54.736178   33734 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:37018->127.0.0.1:32773: read: connection reset by peer
I0717 09:18:58.013914   33734 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0717 09:18:58.013967   33734 ubuntu.go:169] provisioning hostname "minikube"
I0717 09:18:58.014253   33734 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0717 09:18:58.077671   33734 main.go:141] libmachine: Using SSH client type: native
I0717 09:18:58.078104   33734 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0717 09:18:58.078119   33734 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0717 09:18:58.313573   33734 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0717 09:18:58.313662   33734 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0717 09:18:58.338407   33734 main.go:141] libmachine: Using SSH client type: native
I0717 09:18:58.338735   33734 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0717 09:18:58.338763   33734 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0717 09:18:58.505681   33734 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0717 09:18:58.505706   33734 ubuntu.go:175] set auth options {CertDir:/home/aditya/.minikube CaCertPath:/home/aditya/.minikube/certs/ca.pem CaPrivateKeyPath:/home/aditya/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/aditya/.minikube/machines/server.pem ServerKeyPath:/home/aditya/.minikube/machines/server-key.pem ClientKeyPath:/home/aditya/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/aditya/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/aditya/.minikube}
I0717 09:18:58.505728   33734 ubuntu.go:177] setting up certificates
I0717 09:18:58.505739   33734 provision.go:84] configureAuth start
I0717 09:18:58.505839   33734 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0717 09:18:58.535023   33734 provision.go:143] copyHostCerts
I0717 09:18:58.535086   33734 exec_runner.go:144] found /home/aditya/.minikube/ca.pem, removing ...
I0717 09:18:58.535094   33734 exec_runner.go:203] rm: /home/aditya/.minikube/ca.pem
I0717 09:18:58.535181   33734 exec_runner.go:151] cp: /home/aditya/.minikube/certs/ca.pem --> /home/aditya/.minikube/ca.pem (1078 bytes)
I0717 09:18:58.535342   33734 exec_runner.go:144] found /home/aditya/.minikube/cert.pem, removing ...
I0717 09:18:58.535347   33734 exec_runner.go:203] rm: /home/aditya/.minikube/cert.pem
I0717 09:18:58.535396   33734 exec_runner.go:151] cp: /home/aditya/.minikube/certs/cert.pem --> /home/aditya/.minikube/cert.pem (1123 bytes)
I0717 09:18:58.535474   33734 exec_runner.go:144] found /home/aditya/.minikube/key.pem, removing ...
I0717 09:18:58.535478   33734 exec_runner.go:203] rm: /home/aditya/.minikube/key.pem
I0717 09:18:58.535521   33734 exec_runner.go:151] cp: /home/aditya/.minikube/certs/key.pem --> /home/aditya/.minikube/key.pem (1675 bytes)
I0717 09:18:58.535591   33734 provision.go:117] generating server cert: /home/aditya/.minikube/machines/server.pem ca-key=/home/aditya/.minikube/certs/ca.pem private-key=/home/aditya/.minikube/certs/ca-key.pem org=aditya.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0717 09:18:58.760299   33734 provision.go:177] copyRemoteCerts
I0717 09:18:58.760362   33734 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0717 09:18:58.760406   33734 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0717 09:18:58.834293   33734 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/aditya/.minikube/machines/minikube/id_rsa Username:docker}
I0717 09:18:58.989220   33734 ssh_runner.go:362] scp /home/aditya/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0717 09:18:59.150276   33734 ssh_runner.go:362] scp /home/aditya/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I0717 09:18:59.273863   33734 ssh_runner.go:362] scp /home/aditya/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0717 09:18:59.400580   33734 provision.go:87] duration metric: took 894.79896ms to configureAuth
I0717 09:18:59.400639   33734 ubuntu.go:193] setting minikube options for container-runtime
I0717 09:18:59.401428   33734 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0717 09:18:59.401925   33734 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0717 09:18:59.479625   33734 main.go:141] libmachine: Using SSH client type: native
I0717 09:18:59.480325   33734 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0717 09:18:59.480349   33734 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0717 09:18:59.736833   33734 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0717 09:18:59.736849   33734 ubuntu.go:71] root file system type: overlay
I0717 09:18:59.737013   33734 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0717 09:18:59.737126   33734 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0717 09:18:59.773937   33734 main.go:141] libmachine: Using SSH client type: native
I0717 09:18:59.774209   33734 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0717 09:18:59.774283   33734 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0717 09:19:00.003737   33734 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0717 09:19:00.004021   33734 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0717 09:19:00.082165   33734 main.go:141] libmachine: Using SSH client type: native
I0717 09:19:00.083100   33734 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0717 09:19:00.083270   33734 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0717 09:19:00.406357   33734 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0717 09:19:00.406404   33734 machine.go:96] duration metric: took 5.71040844s to provisionDockerMachine
I0717 09:19:00.406439   33734 start.go:293] postStartSetup for "minikube" (driver="docker")
I0717 09:19:00.406558   33734 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0717 09:19:00.406905   33734 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0717 09:19:00.407049   33734 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0717 09:19:00.514288   33734 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/aditya/.minikube/machines/minikube/id_rsa Username:docker}
I0717 09:19:00.779544   33734 ssh_runner.go:195] Run: cat /etc/os-release
I0717 09:19:00.785001   33734 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0717 09:19:00.785043   33734 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0717 09:19:00.785058   33734 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0717 09:19:00.785066   33734 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0717 09:19:00.785079   33734 filesync.go:126] Scanning /home/aditya/.minikube/addons for local assets ...
I0717 09:19:00.785187   33734 filesync.go:126] Scanning /home/aditya/.minikube/files for local assets ...
I0717 09:19:00.785224   33734 start.go:296] duration metric: took 378.77334ms for postStartSetup
I0717 09:19:00.785295   33734 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0717 09:19:00.785349   33734 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0717 09:19:00.816663   33734 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/aditya/.minikube/machines/minikube/id_rsa Username:docker}
I0717 09:19:00.916044   33734 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0717 09:19:00.923192   33734 fix.go:56] duration metric: took 7.14588842s for fixHost
I0717 09:19:00.923211   33734 start.go:83] releasing machines lock for "minikube", held for 7.14593022s
I0717 09:19:00.923312   33734 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0717 09:19:00.947945   33734 ssh_runner.go:195] Run: cat /version.json
I0717 09:19:00.948015   33734 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0717 09:19:00.948074   33734 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0717 09:19:00.948164   33734 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0717 09:19:01.024580   33734 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/aditya/.minikube/machines/minikube/id_rsa Username:docker}
I0717 09:19:01.024830   33734 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/aditya/.minikube/machines/minikube/id_rsa Username:docker}
I0717 09:19:01.421996   33734 ssh_runner.go:195] Run: systemctl --version
I0717 09:19:01.442429   33734 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0717 09:19:01.473911   33734 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0717 09:19:01.542625   33734 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0717 09:19:01.542793   33734 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0717 09:19:01.562356   33734 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0717 09:19:01.562382   33734 start.go:495] detecting cgroup driver to use...
I0717 09:19:01.562422   33734 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0717 09:19:01.562548   33734 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0717 09:19:01.591041   33734 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0717 09:19:01.606540   33734 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0717 09:19:01.623132   33734 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0717 09:19:01.623222   33734 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0717 09:19:01.638496   33734 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0717 09:19:01.652714   33734 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0717 09:19:01.671296   33734 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0717 09:19:01.689691   33734 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0717 09:19:01.711561   33734 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0717 09:19:01.727294   33734 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0717 09:19:01.742618   33734 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0717 09:19:01.765842   33734 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0717 09:19:01.779738   33734 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0717 09:19:01.792490   33734 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0717 09:19:01.942016   33734 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0717 09:19:02.238284   33734 start.go:495] detecting cgroup driver to use...
I0717 09:19:02.238351   33734 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0717 09:19:02.238435   33734 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0717 09:19:02.261130   33734 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0717 09:19:02.261296   33734 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0717 09:19:02.282754   33734 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0717 09:19:02.312643   33734 ssh_runner.go:195] Run: which cri-dockerd
I0717 09:19:02.319110   33734 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0717 09:19:02.338353   33734 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0717 09:19:02.369505   33734 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0717 09:19:02.548358   33734 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0717 09:19:02.714877   33734 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0717 09:19:02.714976   33734 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0717 09:19:02.738673   33734 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0717 09:19:02.753991   33734 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0717 09:19:02.961534   33734 ssh_runner.go:195] Run: sudo systemctl restart docker
I0717 09:19:09.105430   33734 ssh_runner.go:235] Completed: sudo systemctl restart docker: (6.1438476s)
I0717 09:19:09.105609   33734 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0717 09:19:09.128206   33734 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0717 09:19:09.153040   33734 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0717 09:19:09.175502   33734 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0717 09:19:09.360553   33734 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0717 09:19:09.607805   33734 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0717 09:19:09.801978   33734 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0717 09:19:09.825944   33734 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0717 09:19:09.844962   33734 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0717 09:19:10.079962   33734 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0717 09:19:10.202896   33734 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0717 09:19:10.221111   33734 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0717 09:19:10.221193   33734 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0717 09:19:10.226884   33734 start.go:563] Will wait 60s for crictl version
I0717 09:19:10.226943   33734 ssh_runner.go:195] Run: which crictl
I0717 09:19:10.232249   33734 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0717 09:19:10.448226   33734 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0717 09:19:10.448310   33734 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0717 09:19:10.487012   33734 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0717 09:19:10.535942   33734 out.go:235] 🐳  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0717 09:19:10.536346   33734 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0717 09:19:10.576218   33734 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0717 09:19:10.582729   33734 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0717 09:19:10.602635   33734 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0717 09:19:10.652410   33734 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/aditya:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0717 09:19:10.652510   33734 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0717 09:19:10.652575   33734 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0717 09:19:10.700444   33734 docker.go:702] Got preloaded images: -- stdout --
aditya3011/order-service:latest
aditya3011/user-service:latest
aditya3011/order-service:<none>
aditya3011/user-service:<none>
nginx:latest
quay.io/argoproj/argocd:v3.0.6
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/etcd:3.5.21-0
redis:7.2.7-alpine
registry.k8s.io/coredns/coredns:v1.12.0
ghcr.io/dexidp/dex:v2.41.1
registry.k8s.io/pause:3.10
nginx:1.25
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0717 09:19:10.700460   33734 docker.go:632] Images already preloaded, skipping extraction
I0717 09:19:10.700545   33734 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0717 09:19:10.736777   33734 docker.go:702] Got preloaded images: -- stdout --
aditya3011/order-service:latest
aditya3011/user-service:latest
aditya3011/order-service:<none>
aditya3011/user-service:<none>
nginx:latest
quay.io/argoproj/argocd:v3.0.6
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/etcd:3.5.21-0
redis:7.2.7-alpine
registry.k8s.io/coredns/coredns:v1.12.0
ghcr.io/dexidp/dex:v2.41.1
registry.k8s.io/pause:3.10
nginx:1.25
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0717 09:19:10.736793   33734 cache_images.go:84] Images are preloaded, skipping loading
I0717 09:19:10.736803   33734 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.33.1 docker true true} ...
I0717 09:19:10.736912   33734 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0717 09:19:10.736995   33734 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0717 09:19:10.833694   33734 cni.go:84] Creating CNI manager for ""
I0717 09:19:10.833712   33734 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0717 09:19:10.833723   33734 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0717 09:19:10.833762   33734 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0717 09:19:10.833910   33734 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0717 09:19:10.834023   33734 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0717 09:19:10.849065   33734 binaries.go:44] Found k8s binaries, skipping transfer
I0717 09:19:10.849181   33734 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0717 09:19:10.865636   33734 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0717 09:19:10.894371   33734 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0717 09:19:10.922131   33734 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0717 09:19:10.948314   33734 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0717 09:19:10.954110   33734 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0717 09:19:10.971021   33734 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0717 09:19:11.142279   33734 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0717 09:19:11.169005   33734 certs.go:68] Setting up /home/aditya/.minikube/profiles/minikube for IP: 192.168.49.2
I0717 09:19:11.169017   33734 certs.go:194] generating shared ca certs ...
I0717 09:19:11.169096   33734 certs.go:226] acquiring lock for ca certs: {Name:mk95932e6c74a13c4aba0037cdcca5ada7770c3b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0717 09:19:11.169410   33734 certs.go:235] skipping valid "minikubeCA" ca cert: /home/aditya/.minikube/ca.key
I0717 09:19:11.169776   33734 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/aditya/.minikube/proxy-client-ca.key
I0717 09:19:11.169800   33734 certs.go:256] generating profile certs ...
I0717 09:19:11.169972   33734 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/aditya/.minikube/profiles/minikube/client.key
I0717 09:19:11.170049   33734 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/aditya/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0717 09:19:11.170112   33734 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/aditya/.minikube/profiles/minikube/proxy-client.key
I0717 09:19:11.170307   33734 certs.go:484] found cert: /home/aditya/.minikube/certs/ca-key.pem (1675 bytes)
I0717 09:19:11.170350   33734 certs.go:484] found cert: /home/aditya/.minikube/certs/ca.pem (1078 bytes)
I0717 09:19:11.170389   33734 certs.go:484] found cert: /home/aditya/.minikube/certs/cert.pem (1123 bytes)
I0717 09:19:11.170426   33734 certs.go:484] found cert: /home/aditya/.minikube/certs/key.pem (1675 bytes)
I0717 09:19:11.171305   33734 ssh_runner.go:362] scp /home/aditya/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0717 09:19:11.212539   33734 ssh_runner.go:362] scp /home/aditya/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0717 09:19:11.262157   33734 ssh_runner.go:362] scp /home/aditya/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0717 09:19:11.304547   33734 ssh_runner.go:362] scp /home/aditya/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0717 09:19:11.351985   33734 ssh_runner.go:362] scp /home/aditya/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0717 09:19:11.398544   33734 ssh_runner.go:362] scp /home/aditya/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0717 09:19:11.466006   33734 ssh_runner.go:362] scp /home/aditya/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0717 09:19:11.525745   33734 ssh_runner.go:362] scp /home/aditya/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0717 09:19:11.578709   33734 ssh_runner.go:362] scp /home/aditya/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0717 09:19:11.628127   33734 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0717 09:19:11.688784   33734 ssh_runner.go:195] Run: openssl version
I0717 09:19:11.703404   33734 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0717 09:19:11.727736   33734 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0717 09:19:11.735763   33734 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jun 30 10:46 /usr/share/ca-certificates/minikubeCA.pem
I0717 09:19:11.735855   33734 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0717 09:19:11.746804   33734 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0717 09:19:11.774256   33734 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0717 09:19:11.781589   33734 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0717 09:19:11.793226   33734 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0717 09:19:11.805996   33734 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0717 09:19:11.816964   33734 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0717 09:19:11.830661   33734 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0717 09:19:11.842867   33734 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0717 09:19:11.852349   33734 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/aditya:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0717 09:19:11.852516   33734 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0717 09:19:11.900819   33734 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0717 09:19:11.921067   33734 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0717 09:19:11.921079   33734 kubeadm.go:589] restartPrimaryControlPlane start ...
I0717 09:19:11.921181   33734 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0717 09:19:11.938801   33734 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0717 09:19:11.938886   33734 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0717 09:19:11.998959   33734 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /var/jenkins_home/.kube/config
I0717 09:19:11.999013   33734 kubeconfig.go:62] /var/jenkins_home/.kube/config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
W0717 09:19:12.008428   33734 kubeadm.go:612] unable to update kubeconfig (cluster will likely require a reset): write kubeconfig: Error creating directory: /var/jenkins_home/.kube: mkdir /var/jenkins_home: permission denied
I0717 09:19:12.008557   33734 kubeadm.go:593] duration metric: took 87.46859ms to restartPrimaryControlPlane
W0717 09:19:12.008634   33734 out.go:270] 🤦  Unable to restart control-plane node(s), will reset cluster: <no value>
I0717 09:19:12.008728   33734 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force"
I0717 09:19:13.116683   33734 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force": (3.88955127s)
I0717 09:19:13.116895   33734 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0717 09:19:13.141051   33734 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0717 09:19:13.161810   33734 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0717 09:19:13.161920   33734 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0717 09:19:13.182746   33734 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0717 09:19:13.182769   33734 kubeadm.go:157] found existing configuration files:

I0717 09:19:13.182886   33734 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0717 09:19:13.202301   33734 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0717 09:19:13.202438   33734 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0717 09:19:13.221198   33734 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0717 09:19:13.240288   33734 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0717 09:19:13.240410   33734 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0717 09:19:13.259022   33734 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0717 09:19:13.281543   33734 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0717 09:19:13.281658   33734 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0717 09:19:13.302460   33734 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0717 09:19:13.323218   33734 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0717 09:19:13.323332   33734 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0717 09:19:13.341887   33734 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0717 09:19:13.527596   33734 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0717 09:19:13.537334   33734 kubeadm.go:310] 	[WARNING SystemVerification]: cgroups v1 support is in maintenance mode, please migrate to cgroups v2
I0717 09:19:13.727635   33734 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0717 09:19:26.194091   33734 kubeadm.go:310] [init] Using Kubernetes version: v1.33.1
I0717 09:19:26.194179   33734 kubeadm.go:310] [preflight] Running pre-flight checks
I0717 09:19:26.194331   33734 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0717 09:19:26.194599   33734 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0717 09:19:26.194773   33734 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0717 09:19:26.194957   33734 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0717 09:19:26.203236   33734 out.go:235]     ▪ Generating certificates and keys ...
I0717 09:19:26.203389   33734 kubeadm.go:310] [certs] Using existing ca certificate authority
I0717 09:19:26.203451   33734 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0717 09:19:26.203538   33734 kubeadm.go:310] [certs] Using existing apiserver-kubelet-client certificate and key on disk
I0717 09:19:26.203592   33734 kubeadm.go:310] [certs] Using existing front-proxy-ca certificate authority
I0717 09:19:26.203652   33734 kubeadm.go:310] [certs] Using existing front-proxy-client certificate and key on disk
I0717 09:19:26.203698   33734 kubeadm.go:310] [certs] Using existing etcd/ca certificate authority
I0717 09:19:26.203752   33734 kubeadm.go:310] [certs] Using existing etcd/server certificate and key on disk
I0717 09:19:26.203824   33734 kubeadm.go:310] [certs] Using existing etcd/peer certificate and key on disk
I0717 09:19:26.203890   33734 kubeadm.go:310] [certs] Using existing etcd/healthcheck-client certificate and key on disk
I0717 09:19:26.203953   33734 kubeadm.go:310] [certs] Using existing apiserver-etcd-client certificate and key on disk
I0717 09:19:26.203986   33734 kubeadm.go:310] [certs] Using the existing "sa" key
I0717 09:19:26.204033   33734 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0717 09:19:26.204077   33734 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0717 09:19:26.204126   33734 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0717 09:19:26.204171   33734 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0717 09:19:26.204225   33734 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0717 09:19:26.204272   33734 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0717 09:19:26.204342   33734 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0717 09:19:26.204399   33734 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0717 09:19:26.207633   33734 out.go:235]     ▪ Booting up control plane ...
I0717 09:19:26.207846   33734 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0717 09:19:26.207920   33734 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0717 09:19:26.207983   33734 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0717 09:19:26.208083   33734 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0717 09:19:26.208167   33734 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0717 09:19:26.208205   33734 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0717 09:19:26.208395   33734 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0717 09:19:26.208555   33734 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0717 09:19:26.208659   33734 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.01846712s
I0717 09:19:26.208796   33734 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0717 09:19:26.208873   33734 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I0717 09:19:26.208992   33734 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0717 09:19:26.209170   33734 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0717 09:19:26.209338   33734 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 4.11534955s
I0717 09:19:26.209448   33734 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 6.20400293s
I0717 09:19:26.209760   33734 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 8.50611329s
I0717 09:19:26.209937   33734 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0717 09:19:26.210239   33734 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0717 09:19:26.210314   33734 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0717 09:19:26.210690   33734 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0717 09:19:26.210767   33734 kubeadm.go:310] [bootstrap-token] Using token: svuuox.ugn2x15jenx94a4m
I0717 09:19:26.212800   33734 out.go:235]     ▪ Configuring RBAC rules ...
I0717 09:19:26.213177   33734 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0717 09:19:26.213281   33734 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0717 09:19:26.213481   33734 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0717 09:19:26.213592   33734 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0717 09:19:26.213698   33734 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0717 09:19:26.213793   33734 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0717 09:19:26.213903   33734 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0717 09:19:26.213940   33734 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0717 09:19:26.213979   33734 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0717 09:19:26.213982   33734 kubeadm.go:310] 
I0717 09:19:26.214033   33734 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0717 09:19:26.214053   33734 kubeadm.go:310] 
I0717 09:19:26.214125   33734 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0717 09:19:26.214128   33734 kubeadm.go:310] 
I0717 09:19:26.214149   33734 kubeadm.go:310]   mkdir -p $HOME/.kube
I0717 09:19:26.214200   33734 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0717 09:19:26.214244   33734 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0717 09:19:26.214246   33734 kubeadm.go:310] 
I0717 09:19:26.214293   33734 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0717 09:19:26.214295   33734 kubeadm.go:310] 
I0717 09:19:26.214337   33734 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0717 09:19:26.214340   33734 kubeadm.go:310] 
I0717 09:19:26.214385   33734 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0717 09:19:26.214449   33734 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0717 09:19:26.214508   33734 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0717 09:19:26.214511   33734 kubeadm.go:310] 
I0717 09:19:26.214584   33734 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0717 09:19:26.214650   33734 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0717 09:19:26.214652   33734 kubeadm.go:310] 
I0717 09:19:26.214725   33734 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token svuuox.ugn2x15jenx94a4m \
I0717 09:19:26.214814   33734 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:a11394087ac6c93d0c95f4a0ae370696102dd14c53c2a41452633edd3b3c9557 \
I0717 09:19:26.214831   33734 kubeadm.go:310] 	--control-plane 
I0717 09:19:26.214833   33734 kubeadm.go:310] 
I0717 09:19:26.214907   33734 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0717 09:19:26.214910   33734 kubeadm.go:310] 
I0717 09:19:26.214981   33734 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token svuuox.ugn2x15jenx94a4m \
I0717 09:19:26.215080   33734 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:a11394087ac6c93d0c95f4a0ae370696102dd14c53c2a41452633edd3b3c9557 
I0717 09:19:26.215086   33734 cni.go:84] Creating CNI manager for ""
I0717 09:19:26.215096   33734 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0717 09:19:26.220091   33734 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0717 09:19:26.224314   33734 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0717 09:19:26.264407   33734 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0717 09:19:26.309908   33734 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0717 09:19:26.310468   33734 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0717 09:19:26.311126   33734 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_07_17T09_19_26_0700 minikube.k8s.io/version=v1.36.0 minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0717 09:19:26.327397   33734 ops.go:34] apiserver oom_adj: -16
I0717 09:19:26.596212   33734 kubeadm.go:1105] duration metric: took 285.81861ms to wait for elevateKubeSystemPrivileges
I0717 09:19:26.607840   33734 kubeadm.go:394] duration metric: took 17.537173043s to StartCluster
I0717 09:19:26.607975   33734 settings.go:142] acquiring lock: {Name:mk34196cd1454de16269bfe1209d52caa1bd4227 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0717 09:19:26.608156   33734 settings.go:150] Updating kubeconfig:  /var/jenkins_home/.kube/config
I0717 09:19:26.610672   33734 out.go:201] 
W0717 09:19:26.612538   33734 out.go:270] ❌  Exiting due to GUEST_START: failed to start node: Failed kubeconfig update: writing kubeconfig: Error creating directory: /var/jenkins_home/.kube: mkdir /var/jenkins_home: permission denied
W0717 09:19:26.612609   33734 out.go:270] 
W0717 09:19:26.621134   33734 out.go:293] [31m╭───────────────────────────────────────────────────────────────────────────────────────────╮[0m
[31m│[0m                                                                                           [31m│[0m
[31m│[0m    😿  If the above advice does not help, please let us know:                             [31m│[0m
[31m│[0m    👉  https://github.com/kubernetes/minikube/issues/new/choose                           [31m│[0m
[31m│[0m                                                                                           [31m│[0m
[31m│[0m    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    [31m│[0m
[31m│[0m                                                                                           [31m│[0m
[31m╰───────────────────────────────────────────────────────────────────────────────────────────╯[0m
I0717 09:19:26.623482   33734 out.go:201] 


==> Docker <==
Jul 17 03:49:08 minikube dockerd[1233]: time="2025-07-17T03:49:08.658944006Z" level=warning msg="error locating sandbox id ffdf202a8bab37416404a593f7b55fa56fe838a356276bfb8bae0a4396c0a595: sandbox ffdf202a8bab37416404a593f7b55fa56fe838a356276bfb8bae0a4396c0a595 not found"
Jul 17 03:49:08 minikube dockerd[1233]: time="2025-07-17T03:49:08.658996366Z" level=warning msg="error locating sandbox id 6406caa0b8d046dc27119a16dbf1e4b1bd93a1a03ead73ba73be1286766f25c4: sandbox 6406caa0b8d046dc27119a16dbf1e4b1bd93a1a03ead73ba73be1286766f25c4 not found"
Jul 17 03:49:08 minikube dockerd[1233]: time="2025-07-17T03:49:08.659041576Z" level=warning msg="error locating sandbox id fd255d578b829ab28edf5d5b111c40eff473b6963d6beaff7546ce0744d62c82: sandbox fd255d578b829ab28edf5d5b111c40eff473b6963d6beaff7546ce0744d62c82 not found"
Jul 17 03:49:08 minikube dockerd[1233]: time="2025-07-17T03:49:08.659101636Z" level=warning msg="error locating sandbox id bc795fdbac306957ccfe273736069be92ef9aa375c47d990bf7febfa9e1306e4: sandbox bc795fdbac306957ccfe273736069be92ef9aa375c47d990bf7febfa9e1306e4 not found"
Jul 17 03:49:08 minikube dockerd[1233]: time="2025-07-17T03:49:08.659137166Z" level=warning msg="error locating sandbox id e4df29ad1631f07250293bfeee88333663f7d974b9a7420aeed3fd791d5f6e7f: sandbox e4df29ad1631f07250293bfeee88333663f7d974b9a7420aeed3fd791d5f6e7f not found"
Jul 17 03:49:08 minikube dockerd[1233]: time="2025-07-17T03:49:08.659194916Z" level=warning msg="error locating sandbox id 5a22c5d797d12f0a1f37372d06e2ec1f87b232f0b2d71136b87a2587496892f7: sandbox 5a22c5d797d12f0a1f37372d06e2ec1f87b232f0b2d71136b87a2587496892f7 not found"
Jul 17 03:49:08 minikube dockerd[1233]: time="2025-07-17T03:49:08.661441446Z" level=info msg="Loading containers: done."
Jul 17 03:49:08 minikube dockerd[1233]: time="2025-07-17T03:49:08.733244606Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Jul 17 03:49:08 minikube dockerd[1233]: time="2025-07-17T03:49:08.733672616Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Jul 17 03:49:08 minikube dockerd[1233]: time="2025-07-17T03:49:08.733718486Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Jul 17 03:49:08 minikube dockerd[1233]: time="2025-07-17T03:49:08.733740156Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Jul 17 03:49:08 minikube dockerd[1233]: time="2025-07-17T03:49:08.733808246Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Jul 17 03:49:08 minikube dockerd[1233]: time="2025-07-17T03:49:08.734251656Z" level=info msg="Initializing buildkit"
Jul 17 03:49:09 minikube dockerd[1233]: time="2025-07-17T03:49:09.041794496Z" level=info msg="Completed buildkit initialization"
Jul 17 03:49:09 minikube dockerd[1233]: time="2025-07-17T03:49:09.099334836Z" level=info msg="Daemon has completed initialization"
Jul 17 03:49:09 minikube dockerd[1233]: time="2025-07-17T03:49:09.099599606Z" level=info msg="API listen on /var/run/docker.sock"
Jul 17 03:49:09 minikube dockerd[1233]: time="2025-07-17T03:49:09.100026846Z" level=info msg="API listen on [::]:2376"
Jul 17 03:49:09 minikube systemd[1]: Started Docker Application Container Engine.
Jul 17 03:49:10 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jul 17 03:49:10 minikube cri-dockerd[1566]: time="2025-07-17T03:49:10Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Jul 17 03:49:10 minikube cri-dockerd[1566]: time="2025-07-17T03:49:10Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jul 17 03:49:10 minikube cri-dockerd[1566]: time="2025-07-17T03:49:10Z" level=info msg="Start docker client with request timeout 0s"
Jul 17 03:49:10 minikube cri-dockerd[1566]: time="2025-07-17T03:49:10Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jul 17 03:49:10 minikube cri-dockerd[1566]: time="2025-07-17T03:49:10Z" level=info msg="Loaded network plugin cni"
Jul 17 03:49:10 minikube cri-dockerd[1566]: time="2025-07-17T03:49:10Z" level=info msg="Docker cri networking managed by network plugin cni"
Jul 17 03:49:10 minikube cri-dockerd[1566]: time="2025-07-17T03:49:10Z" level=info msg="Setting cgroupDriver cgroupfs"
Jul 17 03:49:10 minikube cri-dockerd[1566]: time="2025-07-17T03:49:10Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jul 17 03:49:10 minikube cri-dockerd[1566]: time="2025-07-17T03:49:10Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jul 17 03:49:10 minikube cri-dockerd[1566]: time="2025-07-17T03:49:10Z" level=info msg="Start cri-dockerd grpc backend"
Jul 17 03:49:10 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-application-controller-0_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d4363fff67970357f8691163c2873f8e695873ca25926d1bdcad9b3c62672525\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-application-controller-0_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5daf7e762a80b823a56301331ec21f1561580ce1233ed28ed3fab0a4efb03f1f\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-application-controller-0_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"509433a043200ce3a1f7b805e9c98bcb262e2e5d245478964e2ef7f7c5643284\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-patch-c4h65_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"119e2a62745fd332569f1622e947f1763a8c8832efa94c21212be79e72baeadb\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-notifications-controller-6c6848bc4c-wdx5w_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ebfbd59bd4c0b32d5b5c83d2e9415f2ffbea748d4a8bc4006a7fae6976ec99b6\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-notifications-controller-6c6848bc4c-wdx5w_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c365eefecb0876a04c370488fc3f13e91ec7244689561d3949d2467200d6d75f\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-notifications-controller-6c6848bc4c-wdx5w_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d7f8f1ad4dab889dfc1b0034be82b0b7f361fa6b46239a38ea3e3d3978af0a55\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-applicationset-controller-655cc58ff8-kt4zj_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"3bfd976ca381e0307e94d92f86c9cdf6ddd98c913b3e6eec8f78b3f524453d05\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-applicationset-controller-655cc58ff8-kt4zj_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"422961ea7a815e0b58ad2e6f911a7390f4bfc65c57a7391b895498bc7b2147aa\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-applicationset-controller-655cc58ff8-kt4zj_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"05f9b3df07c7cc9225d0d5b3dd8c0a05e158d84e8b57248eeccc9e0f6a708b65\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-s2c7d_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"f47919b8ef6f0cc221260c555dbfeae60c1658da29f95b60a204c1b9a4e5997a\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-s2c7d_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5e37c0f0694898798624c81b832a8516a6fed503503becdefa9b8982befd14b0\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-s2c7d_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"58d6f64cd35e898d3a92f1a1747121c93e54e49a0a35498743b9134634e4bdc6\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-controller-67c5cb88f-zsfc9_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"485e137c5f9f760f1499efa1baf75e305b8dede18d60d9c9b0dfbc1a3959e157\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-create-7jv46_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d480f90453501912560456c702719859d50a5b9d6bac49be1dbd98d37b8b6aa5\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-7779f9b69b-64z9l_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d2220259f2a58fa87034074f2b3ba4394ae46934f4d839f4a3107d04183e259f\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-dex-server-7d9dfb4fb8-6thl7_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"2eb46bbdbec70222a1d47436aed0990490a94b66b858fabdbc4065fcc2b4b3d2\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-dex-server-7d9dfb4fb8-6thl7_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"27baa10987472547215e7cb1e4b09b130a6f8b9c4ad59a8dbf4088c4e912af3a\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-server-99c485944-lnz4m_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"74aa6796b9713ec37733402791dbd6aa26761f627f5841f4841afc966385f352\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"argocd-server-99c485944-lnz4m_argocd\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"83e5d356aba25f324f876a0d6ed9c1f08d8f4a86160a6d3c0690c779ff87e8de\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5d59dccf9b-5ftcl_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"36fa7037d43cdf615af5a06cd0dcba7eb600c9e4cbaebc49082bbbec440ded95\""
Jul 17 03:49:12 minikube cri-dockerd[1566]: time="2025-07-17T03:49:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5d59dccf9b-5ftcl_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"75bef2c6211db8e5190fbfe91381373594de76acf42046883fe168c174322998\""
Jul 17 03:49:17 minikube cri-dockerd[1566]: time="2025-07-17T03:49:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c1e10ec0bd232762c2974e05478fbc93b8e3fda1603ae01fd073bcd89200a1b7/resolv.conf as [nameserver 192.168.0.198 options ndots:0]"
Jul 17 03:49:17 minikube cri-dockerd[1566]: time="2025-07-17T03:49:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/68c2e91c3e3c68ced1ceb531f3e37b69ee59cfd589412c1669b0203ab51afd0f/resolv.conf as [nameserver 192.168.0.198 options ndots:0]"
Jul 17 03:49:17 minikube cri-dockerd[1566]: time="2025-07-17T03:49:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7018e9e99086cdcaf0639bab6c4f14dddda74ab9f536faa70ec56b82b115fc34/resolv.conf as [nameserver 192.168.0.198 options ndots:0]"
Jul 17 03:49:17 minikube cri-dockerd[1566]: time="2025-07-17T03:49:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3acb1398c73eed34bb2263fcf3caa2882274780e1a5e92282155ed4fed0548bb/resolv.conf as [nameserver 192.168.0.198 options ndots:0]"
Jul 17 03:49:32 minikube cri-dockerd[1566]: time="2025-07-17T03:49:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5696f061f93e722e3e8175172c1ba0393861a4f61040e7a21be2a8674d21ad6d/resolv.conf as [nameserver 192.168.0.198 options ndots:0]"
Jul 17 03:49:32 minikube cri-dockerd[1566]: time="2025-07-17T03:49:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/22435602b07a38c503ec38d044fab03824e2ec23171a0e80ca81445e802ffcf2/resolv.conf as [nameserver 192.168.0.198 options ndots:0]"
Jul 17 03:49:32 minikube cri-dockerd[1566]: time="2025-07-17T03:49:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3a12c7323fd4b1473b665d995eff70717454238ede6d578df9364a0ac740fe6f/resolv.conf as [nameserver 192.168.0.198 options ndots:0]"
Jul 17 03:49:36 minikube cri-dockerd[1566]: time="2025-07-17T03:49:36Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"


==> container status <==
CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
0bb62de0bae01       b79c189b052cd       About a minute ago   Running             kube-proxy                0                   3a12c7323fd4b       kube-proxy-mmk4r
47dcbf2f7ac01       1cf5f116067c6       About a minute ago   Running             coredns                   0                   22435602b07a3       coredns-674b8bbfcf-xxdk7
740d7d80e50b1       1cf5f116067c6       About a minute ago   Running             coredns                   0                   5696f061f93e7       coredns-674b8bbfcf-6kgcz
9b6781caea56a       398c985c0d950       About a minute ago   Running             kube-scheduler            0                   3acb1398c73ee       kube-scheduler-minikube
8aee98683687e       499038711c081       About a minute ago   Running             etcd                      0                   7018e9e99086c       etcd-minikube
cf2d00d20d241       ef43894fa110c       About a minute ago   Running             kube-controller-manager   0                   68c2e91c3e3c6       kube-controller-manager-minikube
f3fc9b0b4ae91       c6ab243b29f82       About a minute ago   Running             kube-apiserver            0                   c1e10ec0bd232       kube-apiserver-minikube


==> coredns [47dcbf2f7ac0] <==
maxprocs: Leaving GOMAXPROCS=4: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error


==> coredns [740d7d80e50b] <==
maxprocs: Leaving GOMAXPROCS=4: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_07_17T09_19_26_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 17 Jul 2025 03:49:22 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 17 Jul 2025 03:50:59 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 17 Jul 2025 03:49:36 +0000   Thu, 17 Jul 2025 03:49:18 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 17 Jul 2025 03:49:36 +0000   Thu, 17 Jul 2025 03:49:18 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 17 Jul 2025 03:49:36 +0000   Thu, 17 Jul 2025 03:49:18 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 17 Jul 2025 03:49:36 +0000   Thu, 17 Jul 2025 03:49:22 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3939192Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3939192Ki
  pods:               110
System Info:
  Machine ID:                 b6bd7106d09a478097bff2ab3e1544e5
  System UUID:                b6bd7106d09a478097bff2ab3e1544e5
  Boot ID:                    01a0aa2e-b20b-4d0b-a48c-16fbdd7a9a65
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (7 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-674b8bbfcf-6kgcz            100m (2%)     0 (0%)      70Mi (1%)        170Mi (4%)     93s
  kube-system                 coredns-674b8bbfcf-xxdk7            100m (2%)     0 (0%)      70Mi (1%)        170Mi (4%)     93s
  kube-system                 etcd-minikube                       100m (2%)     0 (0%)      100Mi (2%)       0 (0%)         99s
  kube-system                 kube-apiserver-minikube             250m (6%)     0 (0%)      0 (0%)           0 (0%)         97s
  kube-system                 kube-controller-manager-minikube    200m (5%)     0 (0%)      0 (0%)           0 (0%)         97s
  kube-system                 kube-proxy-mmk4r                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         93s
  kube-system                 kube-scheduler-minikube             100m (2%)     0 (0%)      0 (0%)           0 (0%)         97s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%)  0 (0%)
  memory             240Mi (6%)  340Mi (8%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age   From             Message
  ----     ------                             ----  ----             -------
  Normal   Starting                           89s   kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  98s   kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           98s   kubelet          Starting kubelet.
  Warning  CgroupV1                           98s   kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeAllocatableEnforced            97s   kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            97s   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              97s   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               97s   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     94s   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Jul17 02:33] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.000000]  #2 #3
[  +0.000000] PCI: Fatal: No config space access function found
[  +0.020844] PCI: System does not support PCI
[  +0.024583] kvm: no hardware support
[  +0.000005] kvm: no hardware support
[  +0.733841] FS-Cache: Duplicate cookie detected
[  +0.000644] FS-Cache: O-cookie c=00000006 [p=00000002 fl=222 nc=0 na=1]
[  +0.001394] FS-Cache: O-cookie d=00000000488744d5{9P.session} n=00000000b393648b
[  +0.001368] FS-Cache: O-key=[10] '34323934393337333734'
[  +0.000508] FS-Cache: N-cookie c=00000007 [p=00000002 fl=2 nc=0 na=1]
[  +0.000536] FS-Cache: N-cookie d=00000000488744d5{9P.session} n=00000000681465e8
[  +0.001880] FS-Cache: N-key=[10] '34323934393337333734'
[  +1.982710] Failed to connect to bus: No such file or directory
[  +0.045573] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.010650] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001026] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000757] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000932] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001022] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.194629] Failed to connect to bus: No such file or directory
[  +0.322017] systemd-journald[62]: File /var/log/journal/1dd812295b484901a4ffca6a8a1d8127/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +2.382155] WSL (127) ERROR: CheckConnection: getaddrinfo() failed: -5
[Jul17 03:15] tmpfs: Unknown parameter 'noswap'
[Jul17 03:23] hrtimer: interrupt took 10379710 ns
[Jul17 03:42] tmpfs: Unknown parameter 'noswap'
[Jul17 03:43] tmpfs: Unknown parameter 'noswap'
[  +9.277114] tmpfs: Unknown parameter 'noswap'


==> etcd [8aee98683687] <==
{"level":"info","ts":"2025-07-17T03:49:17.995024Z","caller":"etcdserver/server.go:875","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.21","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-07-17T03:49:17.996749Z","caller":"etcdserver/server.go:759","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-07-17T03:49:17.997129Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-17T03:49:18.036355Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-17T03:49:18.036400Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-17T03:49:17.995737Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-17T03:49:18.005607Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-07-17T03:49:18.049364Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-07-17T03:49:18.049923Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"],"added-peer-is-learner":false}
{"level":"info","ts":"2025-07-17T03:49:18.068098Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-07-17T03:49:18.068195Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-07-17T03:49:18.069082Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-17T03:49:18.069276Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-17T03:49:18.253229Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-07-17T03:49:18.253293Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-07-17T03:49:18.253316Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-07-17T03:49:18.253358Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-07-17T03:49:18.253397Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-07-17T03:49:18.253413Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-07-17T03:49:18.253426Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-07-17T03:49:18.257633Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-07-17T03:49:18.257982Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-17T03:49:18.258190Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-17T03:49:18.258012Z","caller":"etcdserver/server.go:2697","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-17T03:49:18.258059Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-07-17T03:49:18.265046Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-07-17T03:49:18.273978Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-17T03:49:18.276317Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-07-17T03:49:18.286724Z","caller":"membership/cluster.go:587","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-17T03:49:18.287252Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-17T03:49:18.288348Z","caller":"etcdserver/server.go:2721","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-07-17T03:49:18.288800Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-17T03:49:18.290705Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"warn","ts":"2025-07-17T03:49:22.394889Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.32112ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/kube-system\" limit:1 ","response":"range_response_count:1 size:350"}
{"level":"info","ts":"2025-07-17T03:49:22.395684Z","caller":"traceutil/trace.go:171","msg":"trace[991905716] range","detail":"{range_begin:/registry/namespaces/kube-system; range_end:; response_count:1; response_revision:14; }","duration":"114.103ms","start":"2025-07-17T03:49:22.281480Z","end":"2025-07-17T03:49:22.395583Z","steps":["trace[991905716] 'agreement among raft nodes before linearized reading'  (duration: 102.58028ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-17T03:49:22.396518Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"115.26317ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-07-17T03:49:22.397193Z","caller":"traceutil/trace.go:171","msg":"trace[1768773089] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:14; }","duration":"115.98664ms","start":"2025-07-17T03:49:22.281169Z","end":"2025-07-17T03:49:22.397155Z","steps":["trace[1768773089] 'agreement among raft nodes before linearized reading'  (duration: 102.91501ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-17T03:49:31.269262Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.92212ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-17T03:49:31.269349Z","caller":"traceutil/trace.go:171","msg":"trace[157340980] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:338; }","duration":"110.02266ms","start":"2025-07-17T03:49:31.159309Z","end":"2025-07-17T03:49:31.269332Z","steps":["trace[157340980] 'range keys from in-memory index tree'  (duration: 109.85722ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-17T03:49:31.270215Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"145.99024ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128038649904627732 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/serviceaccounts/kube-system/default\" mod_revision:0 > success:<request_put:<key:\"/registry/serviceaccounts/kube-system/default\" value_size:112 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2025-07-17T03:49:31.270413Z","caller":"traceutil/trace.go:171","msg":"trace[1903480512] transaction","detail":"{read_only:false; response_revision:340; number_of_response:1; }","duration":"219.24133ms","start":"2025-07-17T03:49:31.051156Z","end":"2025-07-17T03:49:31.270397Z","steps":["trace[1903480512] 'process raft request'  (duration: 219.16015ms)"],"step_count":1}
{"level":"info","ts":"2025-07-17T03:49:31.270612Z","caller":"traceutil/trace.go:171","msg":"trace[1866553309] transaction","detail":"{read_only:false; response_revision:339; number_of_response:1; }","duration":"219.56627ms","start":"2025-07-17T03:49:31.051030Z","end":"2025-07-17T03:49:31.270597Z","steps":["trace[1866553309] 'process raft request'  (duration: 72.46448ms)","trace[1866553309] 'compare'  (duration: 145.7236ms)"],"step_count":2}
{"level":"info","ts":"2025-07-17T03:49:31.277020Z","caller":"traceutil/trace.go:171","msg":"trace[1490400383] transaction","detail":"{read_only:false; response_revision:341; number_of_response:1; }","duration":"198.7722ms","start":"2025-07-17T03:49:31.078222Z","end":"2025-07-17T03:49:31.276994Z","steps":["trace[1490400383] 'process raft request'  (duration: 198.50831ms)"],"step_count":1}
{"level":"info","ts":"2025-07-17T03:49:31.277182Z","caller":"traceutil/trace.go:171","msg":"trace[129681982] linearizableReadLoop","detail":"{readStateIndex:351; appliedIndex:348; }","duration":"188.17524ms","start":"2025-07-17T03:49:31.088993Z","end":"2025-07-17T03:49:31.277169Z","steps":["trace[129681982] 'read index received'  (duration: 34.47103ms)","trace[129681982] 'applied index is now lower than readState.Index'  (duration: 153.70377ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-17T03:49:31.277507Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"188.496ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/endpoint-controller\" limit:1 ","response":"range_response_count:1 size:203"}
{"level":"info","ts":"2025-07-17T03:49:31.277559Z","caller":"traceutil/trace.go:171","msg":"trace[1807820448] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/endpoint-controller; range_end:; response_count:1; response_revision:344; }","duration":"188.59511ms","start":"2025-07-17T03:49:31.088951Z","end":"2025-07-17T03:49:31.277546Z","steps":["trace[1807820448] 'agreement among raft nodes before linearized reading'  (duration: 188.49952ms)"],"step_count":1}
{"level":"info","ts":"2025-07-17T03:49:31.278132Z","caller":"traceutil/trace.go:171","msg":"trace[288967145] transaction","detail":"{read_only:false; response_revision:342; number_of_response:1; }","duration":"182.64983ms","start":"2025-07-17T03:49:31.095449Z","end":"2025-07-17T03:49:31.278099Z","steps":["trace[288967145] 'process raft request'  (duration: 181.412ms)"],"step_count":1}
{"level":"info","ts":"2025-07-17T03:49:31.278410Z","caller":"traceutil/trace.go:171","msg":"trace[918331380] transaction","detail":"{read_only:false; response_revision:343; number_of_response:1; }","duration":"182.77006ms","start":"2025-07-17T03:49:31.095626Z","end":"2025-07-17T03:49:31.278397Z","steps":["trace[918331380] 'process raft request'  (duration: 181.2965ms)"],"step_count":1}
{"level":"info","ts":"2025-07-17T03:49:31.282704Z","caller":"traceutil/trace.go:171","msg":"trace[575865853] transaction","detail":"{read_only:false; response_revision:344; number_of_response:1; }","duration":"185.41545ms","start":"2025-07-17T03:49:31.097262Z","end":"2025-07-17T03:49:31.282678Z","steps":["trace[575865853] 'process raft request'  (duration: 179.73285ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-17T03:49:31.341881Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"211.20407ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/coredns\" limit:1 ","response":"range_response_count:1 size:179"}
{"level":"info","ts":"2025-07-17T03:49:31.341952Z","caller":"traceutil/trace.go:171","msg":"trace[1380902449] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/coredns; range_end:; response_count:1; response_revision:345; }","duration":"211.31891ms","start":"2025-07-17T03:49:31.130619Z","end":"2025-07-17T03:49:31.341938Z","steps":["trace[1380902449] 'agreement among raft nodes before linearized reading'  (duration: 211.19131ms)"],"step_count":1}
{"level":"info","ts":"2025-07-17T03:49:31.465330Z","caller":"traceutil/trace.go:171","msg":"trace[174828180] transaction","detail":"{read_only:false; response_revision:346; number_of_response:1; }","duration":"149.48978ms","start":"2025-07-17T03:49:31.315820Z","end":"2025-07-17T03:49:31.465310Z","steps":["trace[174828180] 'process raft request'  (duration: 138.56953ms)"],"step_count":1}
{"level":"info","ts":"2025-07-17T03:49:31.465652Z","caller":"traceutil/trace.go:171","msg":"trace[1396171126] transaction","detail":"{read_only:false; response_revision:348; number_of_response:1; }","duration":"146.9699ms","start":"2025-07-17T03:49:31.318676Z","end":"2025-07-17T03:49:31.465646Z","steps":["trace[1396171126] 'process raft request'  (duration: 146.62483ms)"],"step_count":1}
{"level":"info","ts":"2025-07-17T03:49:31.465364Z","caller":"traceutil/trace.go:171","msg":"trace[215523392] linearizableReadLoop","detail":"{readStateIndex:358; appliedIndex:355; }","duration":"123.5586ms","start":"2025-07-17T03:49:31.341789Z","end":"2025-07-17T03:49:31.465348Z","steps":["trace[215523392] 'read index received'  (duration: 112.94041ms)","trace[215523392] 'applied index is now lower than readState.Index'  (duration: 10.61764ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-17T03:49:31.465462Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"175.10427ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/kube-system/kube-proxy-mmk4r.1852ed58cbe13293\" limit:1 ","response":"range_response_count:1 size:769"}
{"level":"info","ts":"2025-07-17T03:49:31.466082Z","caller":"traceutil/trace.go:171","msg":"trace[1857201950] range","detail":"{range_begin:/registry/events/kube-system/kube-proxy-mmk4r.1852ed58cbe13293; range_end:; response_count:1; response_revision:348; }","duration":"175.7129ms","start":"2025-07-17T03:49:31.290303Z","end":"2025-07-17T03:49:31.466016Z","steps":["trace[1857201950] 'agreement among raft nodes before linearized reading'  (duration: 175.09151ms)"],"step_count":1}
{"level":"info","ts":"2025-07-17T03:49:31.465624Z","caller":"traceutil/trace.go:171","msg":"trace[1299460766] transaction","detail":"{read_only:false; response_revision:347; number_of_response:1; }","duration":"149.6ms","start":"2025-07-17T03:49:31.316011Z","end":"2025-07-17T03:49:31.465611Z","steps":["trace[1299460766] 'process raft request'  (duration: 149.18805ms)"],"step_count":1}
{"level":"info","ts":"2025-07-17T03:49:31.480351Z","caller":"traceutil/trace.go:171","msg":"trace[1383840883] transaction","detail":"{read_only:false; response_revision:349; number_of_response:1; }","duration":"134.32287ms","start":"2025-07-17T03:49:31.346010Z","end":"2025-07-17T03:49:31.480333Z","steps":["trace[1383840883] 'process raft request'  (duration: 134.24345ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-17T03:49:31.480619Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"137.48185ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/coredns-674b8bbfcf-xxdk7\" limit:1 ","response":"range_response_count:1 size:4800"}
{"level":"info","ts":"2025-07-17T03:49:31.480673Z","caller":"traceutil/trace.go:171","msg":"trace[1870882954] range","detail":"{range_begin:/registry/pods/kube-system/coredns-674b8bbfcf-xxdk7; range_end:; response_count:1; response_revision:349; }","duration":"137.57216ms","start":"2025-07-17T03:49:31.343086Z","end":"2025-07-17T03:49:31.480658Z","steps":["trace[1870882954] 'agreement among raft nodes before linearized reading'  (duration: 137.44918ms)"],"step_count":1}


==> kernel <==
 03:51:03 up  1:18,  0 users,  load average: 1.34, 1.97, 1.56
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [f3fc9b0b4ae9] <==
I0717 03:49:22.061948       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0717 03:49:22.065139       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0717 03:49:22.061965       1 controller.go:78] Starting OpenAPI AggregationController
I0717 03:49:22.061707       1 aggregator.go:169] waiting for initial CRD sync...
I0717 03:49:22.062665       1 controller.go:119] Starting legacy_token_tracking_controller
I0717 03:49:22.066179       1 shared_informer.go:350] "Waiting for caches to sync" controller="configmaps"
I0717 03:49:22.063022       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0717 03:49:22.066753       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0717 03:49:22.063045       1 controller.go:142] Starting OpenAPI controller
I0717 03:49:22.063070       1 controller.go:90] Starting OpenAPI V3 controller
I0717 03:49:22.063083       1 naming_controller.go:299] Starting NamingConditionController
I0717 03:49:22.063095       1 establishing_controller.go:81] Starting EstablishingController
I0717 03:49:22.063108       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0717 03:49:22.082098       1 repairip.go:200] Starting ipallocator-repair-controller
I0717 03:49:22.082144       1 shared_informer.go:350] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0717 03:49:22.082513       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0717 03:49:22.082543       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0717 03:49:22.251437       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0717 03:49:22.251533       1 policy_source.go:240] refreshing policies
I0717 03:49:22.262720       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0717 03:49:22.263082       1 aggregator.go:171] initial CRD sync complete...
I0717 03:49:22.263190       1 autoregister_controller.go:144] Starting autoregister controller
I0717 03:49:22.263226       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0717 03:49:22.263317       1 cache.go:39] Caches are synced for autoregister controller
I0717 03:49:22.264224       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0717 03:49:22.264277       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0717 03:49:22.264920       1 cache.go:39] Caches are synced for LocalAvailability controller
I0717 03:49:22.265524       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0717 03:49:22.268738       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0717 03:49:22.269391       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0717 03:49:22.269680       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0717 03:49:22.270521       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0717 03:49:22.276082       1 controller.go:667] quota admission added evaluator for: namespaces
I0717 03:49:22.282273       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0717 03:49:22.282595       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0717 03:49:22.282647       1 default_servicecidr_controller.go:165] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I0717 03:49:22.338237       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0717 03:49:22.390755       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0717 03:49:22.390973       1 default_servicecidr_controller.go:214] Setting default ServiceCIDR condition Ready to True
I0717 03:49:22.455634       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0717 03:49:22.475985       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0717 03:49:22.475980       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0717 03:49:22.476045       1 default_servicecidr_controller.go:214] Setting default ServiceCIDR condition Ready to True
I0717 03:49:23.109915       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0717 03:49:23.129904       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0717 03:49:23.130117       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0717 03:49:24.284133       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0717 03:49:24.394131       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0717 03:49:24.499459       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0717 03:49:24.510937       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0717 03:49:24.512687       1 controller.go:667] quota admission added evaluator for: endpoints
I0717 03:49:24.520199       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0717 03:49:25.215151       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0717 03:49:25.800916       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0717 03:49:25.836955       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0717 03:49:25.850805       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0717 03:49:29.884389       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0717 03:49:30.332577       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0717 03:49:30.350628       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0717 03:49:30.425757       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps


==> kube-controller-manager [cf2d00d20d24] <==
I0717 03:49:29.475133       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint_slice_mirroring"
I0717 03:49:29.674161       1 controllermanager.go:778] "Started controller" controller="replicationcontroller-controller"
I0717 03:49:29.674391       1 replica_set.go:219] "Starting controller" logger="replicationcontroller-controller" name="replicationcontroller"
I0717 03:49:29.674420       1 shared_informer.go:350] "Waiting for caches to sync" controller="ReplicationController"
I0717 03:49:29.775837       1 controllermanager.go:778] "Started controller" controller="bootstrap-signer-controller"
I0717 03:49:29.775921       1 shared_informer.go:350] "Waiting for caches to sync" controller="bootstrap_signer"
I0717 03:49:29.790919       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0717 03:49:29.815044       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0717 03:49:29.821977       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0717 03:49:29.825539       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0717 03:49:29.825687       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0717 03:49:29.825727       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0717 03:49:29.825807       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0717 03:49:29.825890       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0717 03:49:29.826178       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0717 03:49:29.826246       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0717 03:49:29.827004       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0717 03:49:29.839433       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0717 03:49:29.841828       1 shared_informer.go:357] "Caches are synced" controller="node"
I0717 03:49:29.842068       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0717 03:49:29.842318       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0717 03:49:29.842468       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0717 03:49:29.842665       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0717 03:49:29.844597       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0717 03:49:29.851683       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0717 03:49:29.857219       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0717 03:49:29.869038       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0717 03:49:29.869220       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0717 03:49:29.871214       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0717 03:49:29.872434       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0717 03:49:29.872612       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0717 03:49:29.873850       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0717 03:49:29.876843       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0717 03:49:29.877171       1 shared_informer.go:357] "Caches are synced" controller="job"
I0717 03:49:29.876934       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0717 03:49:29.876976       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0717 03:49:29.877018       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0717 03:49:29.876868       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0717 03:49:29.879649       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0717 03:49:29.880081       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0717 03:49:29.881898       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0717 03:49:29.887244       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0717 03:49:29.890068       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0717 03:49:29.905666       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0717 03:49:29.917794       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0717 03:49:29.951639       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0717 03:49:29.977335       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0717 03:49:30.023329       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0717 03:49:30.076158       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0717 03:49:30.082100       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0717 03:49:30.139029       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0717 03:49:30.175336       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0717 03:49:30.176691       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0717 03:49:30.190794       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0717 03:49:30.193105       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0717 03:49:30.232659       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0717 03:49:30.640573       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0717 03:49:30.681885       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0717 03:49:30.681943       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0717 03:49:30.681958       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-proxy [0bb62de0bae0] <==
I0717 03:49:33.056087       1 server_linux.go:63] "Using iptables proxy"
I0717 03:49:33.286509       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0717 03:49:33.286663       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0717 03:49:33.367663       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0717 03:49:33.367900       1 server_linux.go:145] "Using iptables Proxier"
I0717 03:49:33.380698       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0717 03:49:33.399971       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0717 03:49:33.415994       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0717 03:49:33.416894       1 server.go:516] "Version info" version="v1.33.1"
I0717 03:49:33.417895       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0717 03:49:33.430588       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0717 03:49:33.442931       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0717 03:49:33.452743       1 config.go:199] "Starting service config controller"
I0717 03:49:33.453066       1 config.go:105] "Starting endpoint slice config controller"
I0717 03:49:33.453090       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0717 03:49:33.453126       1 config.go:440] "Starting serviceCIDR config controller"
I0717 03:49:33.453130       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0717 03:49:33.453367       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0717 03:49:33.453973       1 config.go:329] "Starting node config controller"
I0717 03:49:33.454210       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0717 03:49:33.553276       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0717 03:49:33.553223       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0717 03:49:33.554473       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0717 03:49:33.554528       1 shared_informer.go:357] "Caches are synced" controller="service config"


==> kube-scheduler [9b6781caea56] <==
I0717 03:49:20.882460       1 serving.go:386] Generated self-signed cert in-memory
W0717 03:49:22.187357       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0717 03:49:22.187699       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0717 03:49:22.239175       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0717 03:49:22.240021       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0717 03:49:22.384636       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0717 03:49:22.385588       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0717 03:49:22.395569       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0717 03:49:22.400610       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0717 03:49:22.402277       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0717 03:49:22.438578       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0717 03:49:22.455970       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0717 03:49:22.456185       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0717 03:49:22.456522       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0717 03:49:22.456634       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0717 03:49:22.456961       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0717 03:49:22.457255       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0717 03:49:22.467900       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0717 03:49:22.468450       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0717 03:49:22.468791       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0717 03:49:22.468949       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0717 03:49:22.470204       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0717 03:49:22.470573       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0717 03:49:22.470817       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0717 03:49:22.473529       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0717 03:49:22.474004       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0717 03:49:22.474928       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0717 03:49:23.509661       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0717 03:49:23.529973       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0717 03:49:23.626671       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0717 03:49:23.682836       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0717 03:49:23.698313       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0717 03:49:23.700340       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0717 03:49:23.715116       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0717 03:49:23.758673       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0717 03:49:23.786008       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0717 03:49:23.824727       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0717 03:49:23.850571       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0717 03:49:23.877881       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
I0717 03:49:25.401869       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.112052    3244 container_log_manager.go:189] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.112974    3244 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Jul 17 03:49:26 minikube kubelet[3244]: E0717 03:49:26.116757    3244 eviction_manager.go:267] "eviction manager: failed to check if we have separate container filesystem. Ignoring." err="no imagefs label for configured runtime"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.146007    3244 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.146513    3244 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.147246    3244 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.152452    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/3924ef3609584191d8d09190210d2d78-etcd-certs\") pod \"etcd-minikube\" (UID: \"3924ef3609584191d8d09190210d2d78\") " pod="kube-system/etcd-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.153057    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/3924ef3609584191d8d09190210d2d78-etcd-data\") pod \"etcd-minikube\" (UID: \"3924ef3609584191d8d09190210d2d78\") " pod="kube-system/etcd-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.153106    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.152646    3244 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.153156    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.153893    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.154332    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.155062    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: E0717 03:49:26.174186    3244 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.216358    3244 kubelet_node_status.go:75] "Attempting to register node" node="minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.258741    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.260211    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.260843    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.263049    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.263312    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.263504    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.263672    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.263821    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/feee622ba49882ef945e2406d3ba86df-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"feee622ba49882ef945e2406d3ba86df\") " pod="kube-system/kube-scheduler-minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.280635    3244 kubelet_node_status.go:124] "Node was previously registered" node="minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.281291    3244 kubelet_node_status.go:78] "Successfully registered node" node="minikube"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.917093    3244 apiserver.go:52] "Watching apiserver"
Jul 17 03:49:26 minikube kubelet[3244]: I0717 03:49:26.945310    3244 desired_state_of_world_populator.go:158] "Finished populating initial desired state of world"
Jul 17 03:49:27 minikube kubelet[3244]: I0717 03:49:27.138088    3244 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Jul 17 03:49:27 minikube kubelet[3244]: I0717 03:49:27.141232    3244 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Jul 17 03:49:27 minikube kubelet[3244]: E0717 03:49:27.173064    3244 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Jul 17 03:49:27 minikube kubelet[3244]: E0717 03:49:27.173064    3244 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Jul 17 03:49:27 minikube kubelet[3244]: I0717 03:49:27.181696    3244 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=3.1816703410000002 podStartE2EDuration="3.181670341s" podCreationTimestamp="2025-07-17 03:49:24 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-07-17 03:49:27.167599471 +0000 UTC m=+1.407244654" watchObservedRunningTime="2025-07-17 03:49:27.181670341 +0000 UTC m=+1.421315524"
Jul 17 03:49:27 minikube kubelet[3244]: I0717 03:49:27.181896    3244 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.181881871 podStartE2EDuration="1.181881871s" podCreationTimestamp="2025-07-17 03:49:26 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-07-17 03:49:27.181571231 +0000 UTC m=+1.421216524" watchObservedRunningTime="2025-07-17 03:49:27.181881871 +0000 UTC m=+1.421527164"
Jul 17 03:49:27 minikube kubelet[3244]: I0717 03:49:27.194210    3244 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.194181081 podStartE2EDuration="1.194181081s" podCreationTimestamp="2025-07-17 03:49:26 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-07-17 03:49:27.193745151 +0000 UTC m=+1.433390334" watchObservedRunningTime="2025-07-17 03:49:27.194181081 +0000 UTC m=+1.433826374"
Jul 17 03:49:27 minikube kubelet[3244]: I0717 03:49:27.207996    3244 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.207966941 podStartE2EDuration="1.207966941s" podCreationTimestamp="2025-07-17 03:49:26 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-07-17 03:49:27.207837031 +0000 UTC m=+1.447482324" watchObservedRunningTime="2025-07-17 03:49:27.207966941 +0000 UTC m=+1.447612124"
Jul 17 03:49:30 minikube kubelet[3244]: I0717 03:49:30.517312    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-k4r7r\" (UniqueName: \"kubernetes.io/projected/ae6d4798-b8c5-4033-8d18-b1d44de55f88-kube-api-access-k4r7r\") pod \"kube-proxy-mmk4r\" (UID: \"ae6d4798-b8c5-4033-8d18-b1d44de55f88\") " pod="kube-system/kube-proxy-mmk4r"
Jul 17 03:49:30 minikube kubelet[3244]: I0717 03:49:30.518415    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/ae6d4798-b8c5-4033-8d18-b1d44de55f88-kube-proxy\") pod \"kube-proxy-mmk4r\" (UID: \"ae6d4798-b8c5-4033-8d18-b1d44de55f88\") " pod="kube-system/kube-proxy-mmk4r"
Jul 17 03:49:30 minikube kubelet[3244]: I0717 03:49:30.518465    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/ae6d4798-b8c5-4033-8d18-b1d44de55f88-lib-modules\") pod \"kube-proxy-mmk4r\" (UID: \"ae6d4798-b8c5-4033-8d18-b1d44de55f88\") " pod="kube-system/kube-proxy-mmk4r"
Jul 17 03:49:30 minikube kubelet[3244]: I0717 03:49:30.518512    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/ae6d4798-b8c5-4033-8d18-b1d44de55f88-xtables-lock\") pod \"kube-proxy-mmk4r\" (UID: \"ae6d4798-b8c5-4033-8d18-b1d44de55f88\") " pod="kube-system/kube-proxy-mmk4r"
Jul 17 03:49:30 minikube kubelet[3244]: E0717 03:49:30.633848    3244 projected.go:289] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Jul 17 03:49:30 minikube kubelet[3244]: E0717 03:49:30.633932    3244 projected.go:194] Error preparing data for projected volume kube-api-access-k4r7r for pod kube-system/kube-proxy-mmk4r: configmap "kube-root-ca.crt" not found
Jul 17 03:49:30 minikube kubelet[3244]: E0717 03:49:30.634093    3244 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/ae6d4798-b8c5-4033-8d18-b1d44de55f88-kube-api-access-k4r7r podName:ae6d4798-b8c5-4033-8d18-b1d44de55f88 nodeName:}" failed. No retries permitted until 2025-07-17 03:49:31.134055101 +0000 UTC m=+5.373700284 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-k4r7r" (UniqueName: "kubernetes.io/projected/ae6d4798-b8c5-4033-8d18-b1d44de55f88-kube-api-access-k4r7r") pod "kube-proxy-mmk4r" (UID: "ae6d4798-b8c5-4033-8d18-b1d44de55f88") : configmap "kube-root-ca.crt" not found
Jul 17 03:49:30 minikube kubelet[3244]: I0717 03:49:30.926007    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/00f27e35-75fd-4785-9dde-a05f747c2c87-config-volume\") pod \"coredns-674b8bbfcf-6kgcz\" (UID: \"00f27e35-75fd-4785-9dde-a05f747c2c87\") " pod="kube-system/coredns-674b8bbfcf-6kgcz"
Jul 17 03:49:30 minikube kubelet[3244]: I0717 03:49:30.926094    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sdllx\" (UniqueName: \"kubernetes.io/projected/00f27e35-75fd-4785-9dde-a05f747c2c87-kube-api-access-sdllx\") pod \"coredns-674b8bbfcf-6kgcz\" (UID: \"00f27e35-75fd-4785-9dde-a05f747c2c87\") " pod="kube-system/coredns-674b8bbfcf-6kgcz"
Jul 17 03:49:31 minikube kubelet[3244]: I0717 03:49:31.026450    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d3a7a712-929b-407d-b67f-7f67685aee6d-config-volume\") pod \"coredns-674b8bbfcf-xxdk7\" (UID: \"d3a7a712-929b-407d-b67f-7f67685aee6d\") " pod="kube-system/coredns-674b8bbfcf-xxdk7"
Jul 17 03:49:31 minikube kubelet[3244]: I0717 03:49:31.026624    3244 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-29gtw\" (UniqueName: \"kubernetes.io/projected/d3a7a712-929b-407d-b67f-7f67685aee6d-kube-api-access-29gtw\") pod \"coredns-674b8bbfcf-xxdk7\" (UID: \"d3a7a712-929b-407d-b67f-7f67685aee6d\") " pod="kube-system/coredns-674b8bbfcf-xxdk7"
Jul 17 03:49:31 minikube kubelet[3244]: E0717 03:49:31.058190    3244 projected.go:289] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Jul 17 03:49:31 minikube kubelet[3244]: E0717 03:49:31.058280    3244 projected.go:194] Error preparing data for projected volume kube-api-access-sdllx for pod kube-system/coredns-674b8bbfcf-6kgcz: configmap "kube-root-ca.crt" not found
Jul 17 03:49:31 minikube kubelet[3244]: E0717 03:49:31.058373    3244 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/00f27e35-75fd-4785-9dde-a05f747c2c87-kube-api-access-sdllx podName:00f27e35-75fd-4785-9dde-a05f747c2c87 nodeName:}" failed. No retries permitted until 2025-07-17 03:49:31.558343911 +0000 UTC m=+5.797989204 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-sdllx" (UniqueName: "kubernetes.io/projected/00f27e35-75fd-4785-9dde-a05f747c2c87-kube-api-access-sdllx") pod "coredns-674b8bbfcf-6kgcz" (UID: "00f27e35-75fd-4785-9dde-a05f747c2c87") : configmap "kube-root-ca.crt" not found
Jul 17 03:49:31 minikube kubelet[3244]: E0717 03:49:31.287398    3244 projected.go:289] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Jul 17 03:49:31 minikube kubelet[3244]: E0717 03:49:31.287471    3244 projected.go:194] Error preparing data for projected volume kube-api-access-k4r7r for pod kube-system/kube-proxy-mmk4r: configmap "kube-root-ca.crt" not found
Jul 17 03:49:31 minikube kubelet[3244]: E0717 03:49:31.287611    3244 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/ae6d4798-b8c5-4033-8d18-b1d44de55f88-kube-api-access-k4r7r podName:ae6d4798-b8c5-4033-8d18-b1d44de55f88 nodeName:}" failed. No retries permitted until 2025-07-17 03:49:32.287578741 +0000 UTC m=+6.527224034 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "kube-api-access-k4r7r" (UniqueName: "kubernetes.io/projected/ae6d4798-b8c5-4033-8d18-b1d44de55f88-kube-api-access-k4r7r") pod "kube-proxy-mmk4r" (UID: "ae6d4798-b8c5-4033-8d18-b1d44de55f88") : configmap "kube-root-ca.crt" not found
Jul 17 03:49:33 minikube kubelet[3244]: I0717 03:49:33.306543    3244 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-674b8bbfcf-xxdk7" podStartSLOduration=3.306508841 podStartE2EDuration="3.306508841s" podCreationTimestamp="2025-07-17 03:49:30 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-07-17 03:49:33.305239221 +0000 UTC m=+7.544884404" watchObservedRunningTime="2025-07-17 03:49:33.306508841 +0000 UTC m=+7.546154024"
Jul 17 03:49:33 minikube kubelet[3244]: I0717 03:49:33.387713    3244 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-674b8bbfcf-6kgcz" podStartSLOduration=3.387681361 podStartE2EDuration="3.387681361s" podCreationTimestamp="2025-07-17 03:49:30 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-07-17 03:49:33.355517251 +0000 UTC m=+7.595162544" watchObservedRunningTime="2025-07-17 03:49:33.387681361 +0000 UTC m=+7.627326654"
Jul 17 03:49:34 minikube kubelet[3244]: I0717 03:49:34.368099    3244 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Jul 17 03:49:34 minikube kubelet[3244]: I0717 03:49:34.935196    3244 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-mmk4r" podStartSLOduration=4.93516756 podStartE2EDuration="4.93516756s" podCreationTimestamp="2025-07-17 03:49:30 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-07-17 03:49:33.430149391 +0000 UTC m=+7.669794574" watchObservedRunningTime="2025-07-17 03:49:34.93516756 +0000 UTC m=+9.174812853"
Jul 17 03:49:36 minikube kubelet[3244]: I0717 03:49:36.628075    3244 kuberuntime_manager.go:1746] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jul 17 03:49:36 minikube kubelet[3244]: I0717 03:49:36.630636    3244 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Jul 17 03:49:37 minikube kubelet[3244]: I0717 03:49:37.244389    3244 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"

